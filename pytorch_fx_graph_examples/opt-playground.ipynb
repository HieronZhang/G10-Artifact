{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bb6a54f33f4e55972ea9e91b3ce2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForCausalLM were not initialized from the model checkpoint at facebook/opt-6.7b and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# os.environ[\"TORCH_COMPILE_DEBUG\"]=\"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-6.7b\", \n",
    "    load_in_8bit=True, \n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.reset()\n",
    "\n",
    "# g10backend = G10AnalyzerBackend()\n",
    "# resnet50_compiled = torch.compile(resnet50)\n",
    "# resnet50_compiled = resnet50_compiled.to(device)\n",
    "\n",
    "torch.TORCH_LOGS = \"+dynamo,+recompiles\"\n",
    "torch.TORCHDYNAMO_VERBOSE = 1\n",
    "torch._dynamo.config.capture_dynamic_output_shape_ops = True\n",
    "torch._dynamo.config.verbose = True\n",
    "import logging\n",
    "torch._logging.set_logs(dynamo=logging.INFO, graph_breaks=True, schedule=True,trace_call=True)\n",
    "model_compiled = torch.compile(\n",
    "        model,\n",
    "        options={\n",
    "            \"trace.enabled\": True,\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-04 14:42:40,942] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:159\n",
      "[2024-02-04 14:42:40,944] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-04 14:42:40,976] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:40,976] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:40,976] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:40,979] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:281 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:40,979] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             self.input_device = find_device([args, kwargs])\n",
      "[2024-02-04 14:42:40,979] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                 ~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:40,993] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:709 in find_device (find_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:40,993] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             device = find_device(obj)\n",
      "[2024-02-04 14:42:40,993] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:40,996] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:709 in find_device (find_device) (inline depth: 3)\n",
      "[2024-02-04 14:42:40,996] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             device = find_device(obj)\n",
      "[2024-02-04 14:42:40,996] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,001] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,001] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,001] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,004] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,004] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,004] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,006] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 3)\n",
      "[2024-02-04 14:42:41,006] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,006] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,009] [0/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,011] [0/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,012] [0/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:159\n",
      "[2024-02-04 14:42:41,013] [0/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,027] [0/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,034] [1/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:279\n",
      "[2024-02-04 14:42:41,034] [1/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,037] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:281 in pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:41,037] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             self.input_device = find_device([args, kwargs])\n",
      "[2024-02-04 14:42:41,037] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                 ~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,042] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:709 in find_device (find_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,042] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             device = find_device(obj)\n",
      "[2024-02-04 14:42:41,042] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,045] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:709 in find_device (find_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,045] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             device = find_device(obj)\n",
      "[2024-02-04 14:42:41,045] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,050] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:41,050] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,050] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,052] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,052] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,052] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,054] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,054] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,054] [1/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,057] [1/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,059] [1/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,060] [1/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:279\n",
      "[2024-02-04 14:42:41,061] [1/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,064] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:281 in pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:41,064] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             self.input_device = find_device([args, kwargs])\n",
      "[2024-02-04 14:42:41,064] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                 ~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,071] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:709 in find_device (find_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,071] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             device = find_device(obj)\n",
      "[2024-02-04 14:42:41,071] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,074] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call find_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:709 in find_device (find_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,074] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             device = find_device(obj)\n",
      "[2024-02-04 14:42:41,074] [1/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,085] [1/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,091] [2/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:138\n",
      "[2024-02-04 14:42:41,092] [2/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,098] [2/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device)\n",
      "[2024-02-04 14:42:41,098] [2/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,098] [2/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,099] [2/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,099] [2/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,099] [2/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,102] [2/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,102] [2/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:41,102] [2/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,102] [2/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:41,102] [2/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,102] [2/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,102] [2/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,102] [2/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,103] [2/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,105] [2/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:138\n",
      "[2024-02-04 14:42:41,106] [2/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,118] [2/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] WON'T CONVERT <genexpr> /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py line 153\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] ========== TorchDynamo Stack Trace ==========\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] Traceback (most recent call last):\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 727, in _convert_frame\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     result = inner_convert(frame, cache_entry, hooks, frame_state)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 330, in _convert_frame_assert\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     unimplemented(\"generator\")\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/_dynamo/exc.py\", line 193, in unimplemented\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     raise Unsupported(msg)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] torch._dynamo.exc.Unsupported: generator\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] \n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] ========== The above exception occurred while processing the following code ==========\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] \n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"<frozen runpy>\", line 88, in _run_code\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     app.launch_new_instance()\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     app.start()\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     self.io_loop.start()\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     self.asyncio_loop.run_forever()\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     self._run_once()\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     handle._run()\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     self._context.run(self._callback, *self._args)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     await self.process_one()\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     await dispatch(*args)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     await result\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     await super().execute_request(stream, ident, parent)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     reply_content = await reply_content\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     res = shell.run_cell(\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     return super().run_cell(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     result = self._run_cell(\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     result = runner(coro)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     coro.send(None)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     if await self.run_code(code, result, async_=asy):\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/tmp/ipykernel_3346022/3144456629.py\", line 10, in <module>\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     outputs = model_compiled(input_tensor, labels)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     return self._call_impl(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     return forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     return self._call_impl(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     return forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 279, in pre_forward\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     def pre_forward(self, module, *args, **kwargs):\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 138, in send_to_device\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     def send_to_device(tensor, device, non_blocking=False, skip_keys=None):\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] \n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] ==========\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] Traceback (most recent call last):\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 727, in _convert_frame\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     result = inner_convert(frame, cache_entry, hooks, frame_state)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 330, in _convert_frame_assert\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     unimplemented(\"generator\")\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/_dynamo/exc.py\", line 193, in unimplemented\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO]     raise Unsupported(msg)\n",
      "[2024-02-04 14:42:41,124] torch._dynamo.convert_frame: [INFO] torch._dynamo.exc.Unsupported: generator\n",
      "[2024-02-04 14:42:41,126] [3/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153\n",
      "[2024-02-04 14:42:41,127] [3/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,130] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call honor_type from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:152 in resume_in_send_to_device (send_to_device)\n",
      "[2024-02-04 14:42:41,130] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return honor_type(\n",
      "[2024-02-04 14:42:41,130] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,130] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,130] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,130] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,130] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,134] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,134] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:41,134] [3/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,137] [3/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: call_function BuiltinVariable(tuple) [UserDefinedObjectVariable(generator)] {} from user code at:\n",
      "[2024-02-04 14:42:41,137] [3/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 152, in resume_in_send_to_device\n",
      "[2024-02-04 14:42:41,137] [3/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return honor_type(\n",
      "[2024-02-04 14:42:41,137] [3/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 84, in honor_type\n",
      "[2024-02-04 14:42:41,137] [3/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return type(obj)(generator)\n",
      "[2024-02-04 14:42:41,137] [3/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,138] [3/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,140] [3/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153\n",
      "[2024-02-04 14:42:41,141] [3/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,147] [3/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,152] [4/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing honor_type /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:76\n",
      "[2024-02-04 14:42:41,153] [4/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,155] [4/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type)\n",
      "[2024-02-04 14:42:41,155] [4/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:41,155] [4/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,160] [4/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: call_function BuiltinVariable(tuple) [UserDefinedObjectVariable(generator)] {} from user code at:\n",
      "[2024-02-04 14:42:41,160] [4/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 84, in honor_type\n",
      "[2024-02-04 14:42:41,160] [4/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return type(obj)(generator)\n",
      "[2024-02-04 14:42:41,160] [4/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,161] [4/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,162] [4/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing honor_type /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:76\n",
      "[2024-02-04 14:42:41,163] [4/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,165] [4/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type)\n",
      "[2024-02-04 14:42:41,165] [4/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:41,165] [4/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,171] [4/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,176] [2/1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:138\n",
      "[2024-02-04 14:42:41,177] [2/1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,180] [2/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,180] [2/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,180] [2/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,180] [2/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,181] [2/1] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,183] [2/1_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:138\n",
      "[2024-02-04 14:42:41,184] [2/1_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,193] [2/1_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,196] [5/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:166\n",
      "[2024-02-04 14:42:41,197] [5/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,199] [5/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: call_method UserDefinedObjectVariable(_lru_cache_wrapper) __call__ [] {} from user code at:\n",
      "[2024-02-04 14:42:41,199] [5/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 168, in resume_in_send_to_device\n",
      "[2024-02-04 14:42:41,199] [5/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     if is_npu_available() and isinstance(device, int):\n",
      "[2024-02-04 14:42:41,199] [5/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,200] [5/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,201] [5/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:166\n",
      "[2024-02-04 14:42:41,202] [5/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,213] [5/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,216] [6/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_send_to_device /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:168\n",
      "[2024-02-04 14:42:41,217] [6/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,221] [6/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:171 in resume_in_send_to_device (send_to_device)\n",
      "[2024-02-04 14:42:41,221] [6/0] torch._dynamo.output_graph.__trace_call: [DEBUG]             return tensor.to(device, non_blocking=non_blocking)\n",
      "[2024-02-04 14:42:41,221] [6/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,223] [6/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing resume_in_send_to_device (RETURN_VALUE)\n",
      "[2024-02-04 14:42:41,225] [6/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor\n",
      "[2024-02-04 14:42:41,423] [6/0] torch._inductor.debug: [WARNING] model___9 debug trace: /mnt/nvme0n1p1/yuqixue2/g10-gds/torch_compile_debug/run_2024_02_04_14_42_41_422902-pid_3346022/torchinductor/model___9.0\n",
      "[2024-02-04 14:42:41,427] [6/0] torch._dynamo.output_graph: [INFO] Step 2: done compiler function inductor\n",
      "[2024-02-04 14:42:41,430] [6/0] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,435] [7/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297\n",
      "[2024-02-04 14:42:41,436] [7/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,440] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in resume_in_pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:41,440] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,440] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                                             ~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,440] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             kwargs, self.execution_device, skip_keys=self.skip_keys\n",
      "[2024-02-04 14:42:41,440] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,440] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,440] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <dictcomp> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:161 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             {\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 for k, t in tensor.items()\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             }\n",
      "[2024-02-04 14:42:41,444] [7/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,447] [8/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160\n",
      "[2024-02-04 14:42:41,448] [8/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,454] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:165 in resume_in_new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:41,454] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             output = module._old_forward(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,454] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _call_impl from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:944 in forward (OPTForCausalLM.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         outputs = self.model.decoder(\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                   ~~~~~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             input_ids=input_ids,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             attention_mask=attention_mask,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             head_mask=head_mask,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             past_key_values=past_key_values,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             inputs_embeds=inputs_embeds,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             use_cache=use_cache,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             output_attentions=output_attentions,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             output_hidden_states=output_hidden_states,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             return_dict=return_dict,\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,626] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,658] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call new_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520 in _call_impl (Module._call_impl) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,658] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             return forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,658] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,661] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward) (inline depth: 3)\n",
      "[2024-02-04 14:42:41,661] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,661] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,664] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 4)\n",
      "[2024-02-04 14:42:41,664] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,664] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,667] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 5)\n",
      "[2024-02-04 14:42:41,667] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,667] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,669] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call honor_type from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:152 in send_to_device (send_to_device) (inline depth: 5)\n",
      "[2024-02-04 14:42:41,669] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return honor_type(\n",
      "[2024-02-04 14:42:41,669] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,669] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,669] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,669] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,669] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,670] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type) (inline depth: 6)\n",
      "[2024-02-04 14:42:41,670] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:41,670] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,673] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 4)\n",
      "[2024-02-04 14:42:41,673] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,673] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                                             ~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,673] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             kwargs, self.execution_device, skip_keys=self.skip_keys\n",
      "[2024-02-04 14:42:41,673] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,673] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,673] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <dictcomp> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:161 in send_to_device (send_to_device) (inline depth: 5)\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             {\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 for k, t in tensor.items()\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             }\n",
      "[2024-02-04 14:42:41,678] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,680] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:162 in <dictcomp> (send_to_device) (inline depth: 6)\n",
      "[2024-02-04 14:42:41,680] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,680] [8/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                             ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 165, in resume_in_new_forward\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     output = module._old_forward(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 944, in forward\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     outputs = self.model.decoder(\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 161, in send_to_device\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     {\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 162, in <dictcomp>\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,682] [8/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,684] [8/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,685] [8/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160\n",
      "[2024-02-04 14:42:41,686] [8/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,702] [8/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,708] [9/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:849\n",
      "[2024-02-04 14:42:41,708] [9/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _call_impl from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:944 in forward (OPTForCausalLM.forward)\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         outputs = self.model.decoder(\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                   ~~~~~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             input_ids=input_ids,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             attention_mask=attention_mask,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             head_mask=head_mask,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             past_key_values=past_key_values,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             inputs_embeds=inputs_embeds,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             use_cache=use_cache,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             output_attentions=output_attentions,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             output_hidden_states=output_hidden_states,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             return_dict=return_dict,\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,713] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,720] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call new_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520 in _call_impl (Module._call_impl) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,720] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             return forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,720] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,722] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,722] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,722] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,725] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 3)\n",
      "[2024-02-04 14:42:41,725] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,725] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,727] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 4)\n",
      "[2024-02-04 14:42:41,727] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,727] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,729] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call honor_type from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:152 in send_to_device (send_to_device) (inline depth: 4)\n",
      "[2024-02-04 14:42:41,729] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return honor_type(\n",
      "[2024-02-04 14:42:41,729] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,729] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,729] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,729] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,729] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,730] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type) (inline depth: 5)\n",
      "[2024-02-04 14:42:41,730] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:41,730] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,733] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 3)\n",
      "[2024-02-04 14:42:41,733] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,733] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                                             ~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,733] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             kwargs, self.execution_device, skip_keys=self.skip_keys\n",
      "[2024-02-04 14:42:41,733] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,733] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,733] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <dictcomp> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:161 in send_to_device (send_to_device) (inline depth: 4)\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             {\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 for k, t in tensor.items()\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             }\n",
      "[2024-02-04 14:42:41,738] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,740] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:162 in <dictcomp> (send_to_device) (inline depth: 5)\n",
      "[2024-02-04 14:42:41,740] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,740] [9/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                             ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 944, in forward\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     outputs = self.model.decoder(\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 161, in send_to_device\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     {\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 162, in <dictcomp>\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,743] [9/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,744] [9/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,745] [9/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:849\n",
      "[2024-02-04 14:42:41,746] [9/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,761] [9/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,766] [0/1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:159\n",
      "[2024-02-04 14:42:41,767] [0/1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,770] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:41,770] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,770] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,773] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,773] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,773] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,775] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,775] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,775] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,776] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call honor_type from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:152 in send_to_device (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,776] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return honor_type(\n",
      "[2024-02-04 14:42:41,776] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,776] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,776] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,776] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,776] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,777] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type) (inline depth: 3)\n",
      "[2024-02-04 14:42:41,777] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:41,777] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,781] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,781] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,781] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                                             ~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,781] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             kwargs, self.execution_device, skip_keys=self.skip_keys\n",
      "[2024-02-04 14:42:41,781] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,781] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,781] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <dictcomp> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:161 in send_to_device (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             {\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 for k, t in tensor.items()\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             }\n",
      "[2024-02-04 14:42:41,785] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,787] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:162 in <dictcomp> (send_to_device) (inline depth: 3)\n",
      "[2024-02-04 14:42:41,787] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,787] [0/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                             ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 161, in send_to_device\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     {\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 162, in <dictcomp>\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,792] [0/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,794] [0/1] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,800] [0/1_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:159\n",
      "[2024-02-04 14:42:41,801] [0/1_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,807] [0/1_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,812] [1/1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:279\n",
      "[2024-02-04 14:42:41,813] [1/1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,816] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:41,816] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,816] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,819] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,819] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,819] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,820] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call honor_type from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:152 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,820] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return honor_type(\n",
      "[2024-02-04 14:42:41,820] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,820] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,820] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,820] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,820] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,821] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,821] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:41,821] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,825] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:41,825] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,825] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                                             ~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,825] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             kwargs, self.execution_device, skip_keys=self.skip_keys\n",
      "[2024-02-04 14:42:41,825] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,825] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,825] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <dictcomp> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:161 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             {\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 for k, t in tensor.items()\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             }\n",
      "[2024-02-04 14:42:41,830] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:41,832] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:162 in <dictcomp> (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,832] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,832] [1/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                             ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 161, in send_to_device\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     {\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 162, in <dictcomp>\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     k: t if k in skip_keys else send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys)\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,834] [1/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,835] [1/1] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,837] [1/1_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:279\n",
      "[2024-02-04 14:42:41,838] [1/1_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,841] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:41,841] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,841] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,844] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,844] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,844] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,845] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call honor_type from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:152 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,845] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return honor_type(\n",
      "[2024-02-04 14:42:41,845] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~^\n",
      "[2024-02-04 14:42:41,845] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,845] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,845] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:41,845] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:41,846] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,846] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:41,846] [1/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:41,857] [1/1_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,863] [10/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297\n",
      "[2024-02-04 14:42:41,864] [10/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,867] [8/1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160\n",
      "[2024-02-04 14:42:41,868] [8/1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,872] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:165 in resume_in_new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:41,872] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             output = module._old_forward(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,872] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,879] [8/1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call view from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:628 in forward (OPTDecoder.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,879] [8/1] torch._dynamo.output_graph.__trace_call: [DEBUG]             input_ids = input_ids.view(-1, input_shape[-1])\n",
      "[2024-02-04 14:42:41,879] [8/1] torch._dynamo.output_graph.__trace_call: [DEBUG]                         ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,882] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _call_impl from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:635 in forward (OPTDecoder.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,882] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             inputs_embeds = self.embed_tokens(input_ids)\n",
      "[2024-02-04 14:42:41,882] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,893] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call new_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1561 in _call_impl (Module._call_impl) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,893] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             result = forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,893] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,896] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward) (inline depth: 3)\n",
      "[2024-02-04 14:42:41,896] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,896] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,899] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 4)\n",
      "[2024-02-04 14:42:41,899] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,899] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,902] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 5)\n",
      "[2024-02-04 14:42:41,902] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,902] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,904] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 6)\n",
      "[2024-02-04 14:42:41,904] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,904] [8/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 165, in resume_in_new_forward\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     output = module._old_forward(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 635, in forward\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     inputs_embeds = self.embed_tokens(input_ids)\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     result = forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,907] [8/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,909] [8/1] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,910] [8/1_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160\n",
      "[2024-02-04 14:42:41,911] [8/1_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,920] [8/1_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:41,928] [11/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:556\n",
      "[2024-02-04 14:42:41,928] [11/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,933] [11/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call view from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:628 in forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:41,933] [11/0] torch._dynamo.output_graph.__trace_call: [DEBUG]             input_ids = input_ids.view(-1, input_shape[-1])\n",
      "[2024-02-04 14:42:41,933] [11/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                         ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,935] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _call_impl from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:635 in forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:41,935] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             inputs_embeds = self.embed_tokens(input_ids)\n",
      "[2024-02-04 14:42:41,935] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,943] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call new_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1561 in _call_impl (Module._call_impl) (inline depth: 1)\n",
      "[2024-02-04 14:42:41,943] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             result = forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,943] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,945] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward) (inline depth: 2)\n",
      "[2024-02-04 14:42:41,945] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,945] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,948] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 3)\n",
      "[2024-02-04 14:42:41,948] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,948] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,951] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 4)\n",
      "[2024-02-04 14:42:41,951] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,951] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,953] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 5)\n",
      "[2024-02-04 14:42:41,953] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,953] [11/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 635, in forward\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     inputs_embeds = self.embed_tokens(input_ids)\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     result = forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:41,956] [11/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:41,957] [11/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:41,960] [11/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:556\n",
      "[2024-02-04 14:42:41,960] [11/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:41,966] [11/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call view from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:628 in forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:41,966] [11/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]             input_ids = input_ids.view(-1, input_shape[-1])\n",
      "[2024-02-04 14:42:41,966] [11/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                         ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:41,971] [11/0_1] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor\n",
      "[2024-02-04 14:42:42,035] [11/0_1] torch._inductor.debug: [WARNING] model__4_inference_10 debug trace: /tmp/torchinductor_yuqixue2/wz/cwz5yomwioiwsa2h3nklirvyas5jncos6n3si2p353ohlx6rywls.debug\n",
      "[2024-02-04 14:42:42,038] [11/0_1] torch._dynamo.output_graph: [INFO] Step 2: done compiler function inductor\n",
      "[2024-02-04 14:42:42,053] [11/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:42,058] [0/2] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:159\n",
      "[2024-02-04 14:42:42,059] [0/2] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:42,063] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:42,063] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:42,063] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,066] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:42,066] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:42,066] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,068] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:42,068] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:42,068] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,070] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 3)\n",
      "[2024-02-04 14:42:42,070] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:42,070] [0/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:42,073] [0/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:42,074] [0/2] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:42,076] [0/2_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:159\n",
      "[2024-02-04 14:42:42,076] [0/2_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:42,084] [0/2_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:42,090] [1/2] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:279\n",
      "[2024-02-04 14:42:42,091] [1/2] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:42,094] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:42,094] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:42,094] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,098] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:42,098] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:42,098] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,099] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:42,099] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:42,099] [1/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:42,102] [1/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:42,103] [1/2] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:42,104] [1/2_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:279\n",
      "[2024-02-04 14:42:42,105] [1/2_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:42,111] [1/2_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:42,115] [4/1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing honor_type /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:76\n",
      "[2024-02-04 14:42:42,116] [4/1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:42,118] [4/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type)\n",
      "[2024-02-04 14:42:42,118] [4/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:42,118] [4/1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:42,122] [4/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: call_function BuiltinVariable(tuple) [UserDefinedObjectVariable(generator)] {} from user code at:\n",
      "[2024-02-04 14:42:42,122] [4/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 84, in honor_type\n",
      "[2024-02-04 14:42:42,122] [4/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return type(obj)(generator)\n",
      "[2024-02-04 14:42:42,122] [4/1] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:42,123] [4/1] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:42,124] [4/1_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing honor_type /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:76\n",
      "[2024-02-04 14:42:42,125] [4/1_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:42,127] [4/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type)\n",
      "[2024-02-04 14:42:42,127] [4/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:42,127] [4/1_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:42,132] [4/1_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:42,137] [8/2] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160\n",
      "[2024-02-04 14:42:42,137] [8/2] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:42,141] [8/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:165 in resume_in_new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:42,141] [8/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             output = module._old_forward(*args, **kwargs)\n",
      "[2024-02-04 14:42:42,141] [8/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,149] [8/2] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call embedding from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163 in forward (Embedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:42,149] [8/2] torch._dynamo.output_graph.__trace_call: [DEBUG]         return F.embedding(\n",
      "[2024-02-04 14:42:42,149] [8/2] torch._dynamo.output_graph.__trace_call: [DEBUG]                ~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:42,149] [8/2] torch._dynamo.output_graph.__trace_call: [DEBUG]             input, self.weight, self.padding_idx, self.max_norm,\n",
      "[2024-02-04 14:42:42,149] [8/2] torch._dynamo.output_graph.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,149] [8/2] torch._dynamo.output_graph.__trace_call: [DEBUG]             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "[2024-02-04 14:42:42,149] [8/2] torch._dynamo.output_graph.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,152] [8/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call post_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:166 in resume_in_new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:42,152] [8/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return module._hf_hook.post_forward(module, output)\n",
      "[2024-02-04 14:42:42,152] [8/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:42,154] [8/2] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing resume_in_new_forward (RETURN_VALUE)\n",
      "[2024-02-04 14:42:42,156] [8/2] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor\n",
      "[2024-02-04 14:42:42,250] [8/2] torch._inductor.codegen.triton.__schedule: [DEBUG] Schedule:\n",
      "[2024-02-04 14:42:42,250] [8/2] torch._inductor.codegen.triton.__schedule: [DEBUG]  [SchedulerNode(name='buf0')]\n",
      "[2024-02-04 14:42:43,299] [8/2] torch._inductor.debug: [WARNING] model__5_inference_11 debug trace: /tmp/torchinductor_yuqixue2/pe/cpe4mmf7jinzdvcyeop7a46mz5gd6i6ngrz5m7kftg27pqzaf6jv.debug\n",
      "[2024-02-04 14:42:43,302] [8/2] torch._dynamo.output_graph: [INFO] Step 2: done compiler function inductor\n",
      "[2024-02-04 14:42:43,306] [8/2] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:43,313] [12/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_inputs_require_grads /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:1203\n",
      "[2024-02-04 14:42:43,314] [12/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:43,317] [12/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: Tensor.requires_grad_ from user code at:\n",
      "[2024-02-04 14:42:43,317] [12/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1204, in make_inputs_require_grads\n",
      "[2024-02-04 14:42:43,317] [12/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     output.requires_grad_(True)\n",
      "[2024-02-04 14:42:43,317] [12/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:43,318] [12/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:43,319] [12/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing make_inputs_require_grads /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:1203\n",
      "[2024-02-04 14:42:43,320] [12/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:43,325] [12/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:43,328] [13/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_make_inputs_require_grads /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:1204\n",
      "[2024-02-04 14:42:43,329] [13/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:43,333] [14/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:635\n",
      "[2024-02-04 14:42:43,334] [14/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:43,338] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _prepare_decoder_attention_mask from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:650 in resume_in_forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:43,338] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         causal_attention_mask = self._prepare_decoder_attention_mask(\n",
      "[2024-02-04 14:42:43,338] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:43,338] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
      "[2024-02-04 14:42:43,338] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,338] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:43,338] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _make_causal_mask from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:538 in _prepare_decoder_attention_mask (OPTDecoder._prepare_decoder_attention_mask) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             combined_attention_mask = _make_causal_mask(\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                       ~~~~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 input_shape,\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 inputs_embeds.dtype,\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 device=inputs_embeds.device,\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 past_key_values_length=past_key_values_length,\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             )\n",
      "[2024-02-04 14:42:43,340] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:43,342] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call full from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:74 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,342] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
      "[2024-02-04 14:42:43,342] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,345] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call arange from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:75 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,345] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask_cond = torch.arange(mask.size(-1), device=device)\n",
      "[2024-02-04 14:42:43,345] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,348] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call add from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:76 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,348] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "[2024-02-04 14:42:43,348] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                                    ~~~~~~~~~~^~~\n",
      "[2024-02-04 14:42:43,350] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call view from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:76 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,350] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "[2024-02-04 14:42:43,350] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                                   ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,352] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call lt from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:76 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,352] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "[2024-02-04 14:42:43,352] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,354] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call masked_fill_ from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:76 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,354] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "[2024-02-04 14:42:43,354] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,356] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:77 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,356] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask = mask.to(dtype)\n",
      "[2024-02-04 14:42:43,356] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~~~~^^^^^^^\n",
      "[2024-02-04 14:42:43,358] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call getitem from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:81 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,358] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
      "[2024-02-04 14:42:43,358] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,362] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call expand from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:81 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,362] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
      "[2024-02-04 14:42:43,362] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,365] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _expand_mask from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:547 in _prepare_decoder_attention_mask (OPTDecoder._prepare_decoder_attention_mask) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,365] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
      "[2024-02-04 14:42:43,365] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,366] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call getitem_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:91 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,366] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "[2024-02-04 14:42:43,366] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,369] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call expand_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:91 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,369] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "[2024-02-04 14:42:43,369] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,371] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:91 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,371] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "[2024-02-04 14:42:43,371] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "[2024-02-04 14:42:43,373] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call sub from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:93 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,373] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     inverted_mask = 1.0 - expanded_mask\n",
      "[2024-02-04 14:42:43,373] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~^~~~~~~~~~~~~~~\n",
      "[2024-02-04 14:42:43,378] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to_2 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:95 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,378] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
      "[2024-02-04 14:42:43,378] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                                      ~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,380] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call masked_fill from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:95 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,380] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
      "[2024-02-04 14:42:43,380] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,382] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to_3 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:547 in _prepare_decoder_attention_mask (OPTDecoder._prepare_decoder_attention_mask) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,382] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]             expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
      "[2024-02-04 14:42:43,382] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:43,382] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                 inputs_embeds.device\n",
      "[2024-02-04 14:42:43,382] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,382] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]             )\n",
      "[2024-02-04 14:42:43,382] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:43,384] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call add_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:551 in _prepare_decoder_attention_mask (OPTDecoder._prepare_decoder_attention_mask) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,384] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                 expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
      "[2024-02-04 14:42:43,384] [14/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                                                                            ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[2024-02-04 14:42:43,387] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _call_impl from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:653 in resume_in_forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:43,387] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         pos_embeds = self.embed_positions(attention_mask, past_key_values_length)\n",
      "[2024-02-04 14:42:43,387] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,394] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call new_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520 in _call_impl (Module._call_impl) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,394] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             return forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:43,394] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,397] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,397] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:43,397] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,401] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 3)\n",
      "[2024-02-04 14:42:43,401] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:43,401] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,407] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 4)\n",
      "[2024-02-04 14:42:43,407] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:43,407] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,409] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 5)\n",
      "[2024-02-04 14:42:43,409] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:43,409] [14/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 653, in resume_in_forward\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     pos_embeds = self.embed_positions(attention_mask, past_key_values_length)\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return forward_call(*args, **kwargs)\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:43,412] [14/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:43,413] [14/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:43,416] [14/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:635\n",
      "[2024-02-04 14:42:43,417] [14/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:43,422] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _prepare_decoder_attention_mask from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:650 in resume_in_forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:43,422] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         causal_attention_mask = self._prepare_decoder_attention_mask(\n",
      "[2024-02-04 14:42:43,422] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:43,422] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
      "[2024-02-04 14:42:43,422] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,422] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         )\n",
      "[2024-02-04 14:42:43,422] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ^\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _make_causal_mask from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:538 in _prepare_decoder_attention_mask (OPTDecoder._prepare_decoder_attention_mask) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             combined_attention_mask = _make_causal_mask(\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                       ~~~~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 input_shape,\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 inputs_embeds.dtype,\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 device=inputs_embeds.device,\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 past_key_values_length=past_key_values_length,\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             )\n",
      "[2024-02-04 14:42:43,424] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:43,427] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call full from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:74 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,427] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
      "[2024-02-04 14:42:43,427] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,429] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call arange from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:75 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,429] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask_cond = torch.arange(mask.size(-1), device=device)\n",
      "[2024-02-04 14:42:43,429] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                 ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,432] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call add from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:76 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,432] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "[2024-02-04 14:42:43,432] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                                    ~~~~~~~~~~^~~\n",
      "[2024-02-04 14:42:43,434] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call view from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:76 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,434] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "[2024-02-04 14:42:43,434] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                                   ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,436] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call lt from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:76 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,436] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "[2024-02-04 14:42:43,436] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,438] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call masked_fill_ from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:76 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,438] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
      "[2024-02-04 14:42:43,438] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,439] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:77 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,439] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     mask = mask.to(dtype)\n",
      "[2024-02-04 14:42:43,439] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~~~~^^^^^^^\n",
      "[2024-02-04 14:42:43,441] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call getitem from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:81 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,441] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
      "[2024-02-04 14:42:43,441] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,445] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call expand from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:81 in _make_causal_mask (_make_causal_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,445] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
      "[2024-02-04 14:42:43,445] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,447] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call _expand_mask from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:547 in _prepare_decoder_attention_mask (OPTDecoder._prepare_decoder_attention_mask) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,447] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
      "[2024-02-04 14:42:43,447] [14/0_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                                  ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,448] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call getitem_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:91 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,448] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "[2024-02-04 14:42:43,448] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,451] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call expand_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:91 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,451] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "[2024-02-04 14:42:43,451] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,453] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:91 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,453] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
      "[2024-02-04 14:42:43,453] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "[2024-02-04 14:42:43,455] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call sub from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:93 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,455] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     inverted_mask = 1.0 - expanded_mask\n",
      "[2024-02-04 14:42:43,455] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~^~~~~~~~~~~~~~~\n",
      "[2024-02-04 14:42:43,458] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to_2 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:95 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,458] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
      "[2024-02-04 14:42:43,458] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                                      ~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,460] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call masked_fill from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:95 in _expand_mask (_expand_mask) (inline depth: 2)\n",
      "[2024-02-04 14:42:43,460] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]     return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
      "[2024-02-04 14:42:43,460] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]            ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,462] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call to_3 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:547 in _prepare_decoder_attention_mask (OPTDecoder._prepare_decoder_attention_mask) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,462] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]             expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
      "[2024-02-04 14:42:43,462] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:43,462] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                 inputs_embeds.device\n",
      "[2024-02-04 14:42:43,462] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                 ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:43,462] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]             )\n",
      "[2024-02-04 14:42:43,462] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]             ^\n",
      "[2024-02-04 14:42:43,463] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call add_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:551 in _prepare_decoder_attention_mask (OPTDecoder._prepare_decoder_attention_mask) (inline depth: 1)\n",
      "[2024-02-04 14:42:43,463] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                 expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
      "[2024-02-04 14:42:43,463] [14/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                                                                            ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[2024-02-04 14:42:43,468] [14/0_1] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor\n",
      "[2024-02-04 14:42:43,673] [14/0_1] torch._inductor.codegen.triton.__schedule: [DEBUG] Schedule:\n",
      "[2024-02-04 14:42:43,673] [14/0_1] torch._inductor.codegen.triton.__schedule: [DEBUG]  [SchedulerNode(name='buf0')]\n",
      "[2024-02-04 14:42:44,248] [14/0_1] torch._inductor.debug: [WARNING] model__6_inference_12 debug trace: /tmp/torchinductor_yuqixue2/up/cup2pclb5qqq55sj5sxcph3txg53dmgfjea3wdjmmeouq73fzu4y.debug\n",
      "[2024-02-04 14:42:44,252] [14/0_1] torch._dynamo.output_graph: [INFO] Step 2: done compiler function inductor\n",
      "[2024-02-04 14:42:44,278] [14/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:44,285] [0/3] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:159\n",
      "[2024-02-04 14:42:44,286] [0/3] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:44,290] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call pre_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160 in new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:44,290] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:44,290] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,293] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,293] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:44,293] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,295] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:44,295] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:44,295] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,297] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 3)\n",
      "[2024-02-04 14:42:44,297] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:44,297] [0/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 160, in new_forward\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:44,303] [0/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:44,304] [0/3] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:44,305] [0/3_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:159\n",
      "[2024-02-04 14:42:44,306] [0/3_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:44,313] [0/3_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:44,319] [1/3] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:279\n",
      "[2024-02-04 14:42:44,319] [1/3] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:44,322] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:297 in pre_forward (AlignDevicesHook.pre_forward)\n",
      "[2024-02-04 14:42:44,322] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:44,322] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,325] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call <genexpr> from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in send_to_device (send_to_device) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,325] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:44,325] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,326] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call send_to_device from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:153 in <genexpr> (send_to_device) (inline depth: 2)\n",
      "[2024-02-04 14:42:44,326] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:44,326] [1/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: hasattr TensorVariable to from user code at:\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 297, in pre_forward\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return send_to_device(args, self.execution_device), send_to_device(\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in send_to_device\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 153, in <genexpr>\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 166, in send_to_device\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     elif hasattr(tensor, \"to\"):\n",
      "[2024-02-04 14:42:44,328] [1/3] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:44,329] [1/3] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:44,331] [1/3_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing pre_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:279\n",
      "[2024-02-04 14:42:44,331] [1/3_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:44,337] [1/3_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:44,342] [4/2] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing honor_type /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:76\n",
      "[2024-02-04 14:42:44,343] [4/2] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:44,345] [4/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type)\n",
      "[2024-02-04 14:42:44,345] [4/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:44,345] [4/2] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:44,348] [4/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: call_function BuiltinVariable(tuple) [UserDefinedObjectVariable(generator)] {} from user code at:\n",
      "[2024-02-04 14:42:44,348] [4/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py\", line 84, in honor_type\n",
      "[2024-02-04 14:42:44,348] [4/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return type(obj)(generator)\n",
      "[2024-02-04 14:42:44,348] [4/2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:44,349] [4/2] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:44,350] [4/2_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing honor_type /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:76\n",
      "[2024-02-04 14:42:44,350] [4/2_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:44,352] [4/2_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call is_namedtuple from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:81 in honor_type (honor_type)\n",
      "[2024-02-04 14:42:44,352] [4/2_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]     if is_namedtuple(obj):\n",
      "[2024-02-04 14:42:44,352] [4/2_1] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]        ~~~~~~~~~~~~~^^^^^\n",
      "[2024-02-04 14:42:44,357] [4/2_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:44,362] [8/3] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_new_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160\n",
      "[2024-02-04 14:42:44,362] [8/3] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:44,366] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:165 in resume_in_new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:44,366] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]             output = module._old_forward(*args, **kwargs)\n",
      "[2024-02-04 14:42:44,366] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,367] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call long from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:111 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,367] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         attention_mask = attention_mask.long()\n",
      "[2024-02-04 14:42:44,367] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                          ~~~~~~~~~~~~~~~~~~~^^\n",
      "[2024-02-04 14:42:44,369] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call cumsum from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:114 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,369] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n",
      "[2024-02-04 14:42:44,369] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                      ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,372] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call type_as from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:114 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,372] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n",
      "[2024-02-04 14:42:44,372] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,374] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call mul from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:114 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,374] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n",
      "[2024-02-04 14:42:44,374] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\n",
      "[2024-02-04 14:42:44,375] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call long_1 from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:114 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,375] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n",
      "[2024-02-04 14:42:44,375] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "[2024-02-04 14:42:44,377] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call sub from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:114 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,377] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n",
      "[2024-02-04 14:42:44,377] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~\n",
      "[2024-02-04 14:42:44,379] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call getitem from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:117 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,379] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         positions = positions[:, past_key_values_length:]\n",
      "[2024-02-04 14:42:44,379] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                     ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,382] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call add from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:119 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,382] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         return super().forward(positions + self.offset)\n",
      "[2024-02-04 14:42:44,382] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                                ~~~~~~~~~~^~~~~~~~~~~~~\n",
      "[2024-02-04 14:42:44,383] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:119 in forward (OPTLearnedPositionalEmbedding.forward) (inline depth: 1)\n",
      "[2024-02-04 14:42:44,383] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return super().forward(positions + self.offset)\n",
      "[2024-02-04 14:42:44,383] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,387] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call embedding from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163 in forward (Embedding.forward) (inline depth: 2)\n",
      "[2024-02-04 14:42:44,387] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]         return F.embedding(\n",
      "[2024-02-04 14:42:44,387] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]                ~~~~~~~~~~~^\n",
      "[2024-02-04 14:42:44,387] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]             input, self.weight, self.padding_idx, self.max_norm,\n",
      "[2024-02-04 14:42:44,387] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,387] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "[2024-02-04 14:42:44,387] [8/3] torch._dynamo.output_graph.__trace_call: [DEBUG]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,389] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call post_forward from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:166 in resume_in_new_forward (add_hook_to_module.new_forward)\n",
      "[2024-02-04 14:42:44,389] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         return module._hf_hook.post_forward(module, output)\n",
      "[2024-02-04 14:42:44,389] [8/3] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:44,391] [8/3] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing resume_in_new_forward (RETURN_VALUE)\n",
      "[2024-02-04 14:42:44,393] [8/3] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor\n",
      "[2024-02-04 14:42:44,448] [8/3] torch._inductor.codegen.triton.__schedule: [DEBUG] Schedule:\n",
      "[2024-02-04 14:42:44,448] [8/3] torch._inductor.codegen.triton.__schedule: [DEBUG]  [SchedulerNode(name='buf2')]\n",
      "[2024-02-04 14:42:45,048] [8/3] torch._inductor.debug: [WARNING] model__7_inference_13 debug trace: /tmp/torchinductor_yuqixue2/l2/cl2rlxzrdwwqk4gfnvmfnwe3qgqnknxuf4he5armmkhx432wr3av.debug\n",
      "[2024-02-04 14:42:45,052] [8/3] torch._dynamo.output_graph: [INFO] Step 2: done compiler function inductor\n",
      "[2024-02-04 14:42:45,055] [8/3] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "[2024-02-04 14:42:45,068] [15/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:653\n",
      "[2024-02-04 14:42:45,069] [15/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:45,073] [15/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call add from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:658 in resume_in_forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:45,073] [15/0] torch._dynamo.output_graph.__trace_call: [DEBUG]         hidden_states = inputs_embeds + pos_embeds\n",
      "[2024-02-04 14:42:45,073] [15/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                         ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "[2024-02-04 14:42:45,077] [15/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: call_method UserDefinedObjectVariable(Logger) warning_once [ConstantVariable(str)] {} from user code at:\n",
      "[2024-02-04 14:42:45,077] [15/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File \"/home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py\", line 662, in resume_in_forward\n",
      "[2024-02-04 14:42:45,077] [15/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     logger.warning_once(\n",
      "[2024-02-04 14:42:45,077] [15/0] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] \n",
      "[2024-02-04 14:42:45,078] [15/0] torch._dynamo.convert_frame: [INFO] Restarting analysis due to _dynamo/symbolic_convert.py:149 in fail_and_restart_analysis\n",
      "[2024-02-04 14:42:45,081] [15/0_1] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:653\n",
      "[2024-02-04 14:42:45,082] [15/0_1] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:45,086] [15/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call add from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:658 in resume_in_forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:45,086] [15/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]         hidden_states = inputs_embeds + pos_embeds\n",
      "[2024-02-04 14:42:45,086] [15/0_1] torch._dynamo.output_graph.__trace_call: [DEBUG]                         ~~~~~~~~~~~~~~^~~~~~~~~~~~\n",
      "[2024-02-04 14:42:45,091] [15/0_1] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function inductor\n",
      "[2024-02-04 14:42:45,184] [15/0_1] torch._inductor.codegen.triton.__schedule: [DEBUG] Schedule:\n",
      "[2024-02-04 14:42:45,184] [15/0_1] torch._inductor.codegen.triton.__schedule: [DEBUG]  [SchedulerNode(name='buf0')]\n",
      "[2024-02-04 14:42:45,747] [15/0_1] torch._inductor.debug: [WARNING] model__8_forward_15 debug trace: /tmp/torchinductor_yuqixue2/xk/cxkr76tzsdcqv3n7x4dyzoqswjz6wkf6ynehpajdclij6nvtppjn.debug\n",
      "[2024-02-04 14:42:45,751] [15/0_1] torch._dynamo.output_graph: [INFO] Step 2: done compiler function inductor\n",
      "[2024-02-04 14:42:45,777] [15/0_1] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "[2024-02-04 14:42:45,787] [16/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing resume_in_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:662\n",
      "[2024-02-04 14:42:45,788] [16/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "[2024-02-04 14:42:45,796] [16/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call rand from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:687 in resume_in_forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:45,796] [16/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                 dropout_probability = torch.rand([])\n",
      "[2024-02-04 14:42:45,796] [16/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                                       ~~~~~~~~~~^^^^\n",
      "[2024-02-04 14:42:45,798] [16/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call lt from /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:688 in resume_in_forward (OPTDecoder.forward)\n",
      "[2024-02-04 14:42:45,798] [16/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                 if dropout_probability < self.layerdrop:\n",
      "[2024-02-04 14:42:45,798] [16/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-02-04 14:42:45,800] [16/0] torch._dynamo.symbolic_convert: [INFO] Skipping frame because there is a graph break in a for/while loop\n",
      "[2024-02-04 14:42:45,800] [16/0] torch._dynamo.symbolic_convert: [INFO] <FrameSummary file /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py, line 688 in resume_in_forward>\n",
      "[2024-02-04 14:42:45,801] [17/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing create_custom_forward /home/yuqixue2/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:695\n",
      "[2024-02-04 14:42:45,802] [17/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env\n",
      "/home/yuqixue2/miniconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Half but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_compiled(input_tensor, labels)\n\u001b[1;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(outputs\u001b[38;5;241m.\u001b[39mpooler_output, outputs\u001b[38;5;241m.\u001b[39mpooler_output)\n\u001b[1;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36mresume_in_new_forward\u001b[0;34m(___stack0, module)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:849\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_decoder\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\n\u001b[0;32m--> 849\u001b[0m \u001b[38;5;129m@replace_return_docstrings\u001b[39m(output_type\u001b[38;5;241m=\u001b[39mCausalLMOutputWithPast, config_class\u001b[38;5;241m=\u001b[39m_CONFIG_FOR_DOC)\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    852\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    853\u001b[0m     attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    854\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m     past_key_values: Optional[List[torch\u001b[38;5;241m.\u001b[39mFloatTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    856\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    857\u001b[0m     labels: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    858\u001b[0m     use_cache: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    859\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    860\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    861\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    862\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m    863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious. I'm just a little bit of a weirdo.\"\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m     output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:160\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36mresume_in_new_forward\u001b[0;34m(___stack0, module)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:556\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    550\u001b[0m         combined_attention_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    551\u001b[0m             expanded_attn_mask \u001b[38;5;28;01mif\u001b[39;00m combined_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m expanded_attn_mask \u001b[38;5;241m+\u001b[39m combined_attention_mask\n\u001b[1;32m    552\u001b[0m         )\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_attention_mask\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    558\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    559\u001b[0m     attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    560\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    561\u001b[0m     past_key_values: Optional[List[torch\u001b[38;5;241m.\u001b[39mFloatTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    562\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    563\u001b[0m     use_cache: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    564\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    565\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, BaseModelOutputWithPast]:\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    615\u001b[0m     output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:635\u001b[0m, in \u001b[0;36mresume_in_forward\u001b[0;34m(___stack0, self, attention_mask, head_mask, past_key_values, use_cache, output_hidden_states, return_dict, input_shape)\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either decoder_input_ids or decoder_inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 635\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[1;32m    637\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m    638\u001b[0m past_key_values_length \u001b[38;5;241m=\u001b[39m past_key_values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:653\u001b[0m, in \u001b[0;36mresume_in_forward\u001b[0;34m(___stack0, self, head_mask, past_key_values, use_cache, output_hidden_states, return_dict, inputs_embeds, causal_attention_mask)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided attention mask has length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but its length should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_seq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (sum of the lengths of current and past inputs)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m     )\n\u001b[1;32m    650\u001b[0m causal_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_decoder_attention_mask(\n\u001b[1;32m    651\u001b[0m     attention_mask, input_shape, inputs_embeds, past_key_values_length\n\u001b[1;32m    652\u001b[0m )\n\u001b[0;32m--> 653\u001b[0m pos_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_positions(attention_mask, past_key_values_length)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_in(inputs_embeds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:702\u001b[0m, in \u001b[0;36mresume_in_forward\u001b[0;34m(___stack0, self, head_mask, past_key_values, output_hidden_states, return_dict, causal_attention_mask, hidden_states)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m module(\u001b[38;5;241m*\u001b[39minputs, output_attentions, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_forward\n\u001b[0;32m--> 702\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    703\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    704\u001b[0m         hidden_states,\n\u001b[1;32m    705\u001b[0m         causal_attention_mask,\n\u001b[1;32m    706\u001b[0m         head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    708\u001b[0m     )\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    711\u001b[0m         hidden_states,\n\u001b[1;32m    712\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:482\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m         )\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CheckpointFunction\u001b[38;5;241m.\u001b[39mapply(function, preserve, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    485\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    486\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:261\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 261\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m run_function(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:698\u001b[0m, in \u001b[0;36mOPTDecoder.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_forward\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# None for past_key_value\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module(\u001b[38;5;241m*\u001b[39minputs, output_attentions, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:327\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_layer_norm_before:\n\u001b[0;32m--> 327\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    330\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    331\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    332\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    336\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlayer_norm(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlayer_norm(\u001b[38;5;28minput\u001b[39m, normalized_shape, weight, bias, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Half but found Float"
     ]
    }
   ],
   "source": [
    "input_tensor = model.dummy_inputs[\"input_ids\"].clone().to(\"cuda\")\n",
    "labels = input_tensor.clone().to(\"cuda\")\n",
    "targets = input_tensor.clone().to(\"cuda\")\n",
    "\n",
    "# Pre-train the model\n",
    "model_compiled.train()\n",
    "\n",
    "for epoch in range(2):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_compiled(input_tensor, labels)\n",
    "    loss = loss_function(outputs.pooler_output, outputs.pooler_output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save the pre-trained model\n",
    "# model.save_pretrained(\"my_bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
    "\n",
    "\n",
    "# bknd = G10AnalyzerBackend()\n",
    "# torch._dynamo.register_backend(bknd, \"g10\")\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=data['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4, \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=1,#100, \n",
    "        max_steps=2,#200, \n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        output_dir='outputs',\n",
    "        torch_compile=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
